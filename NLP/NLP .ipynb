{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5707f5b6",
   "metadata": {},
   "source": [
    "# Natural Language Processing     \n",
    "Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a870ce7",
   "metadata": {},
   "source": [
    "Natural Language Processing (or NLP) is the science of dealing with human language or text data. One of the NLP application is Topic identification, which is a technique used to discover topics across text documents.\n",
    "We will learn how to identify topics from texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52704dcd",
   "metadata": {},
   "source": [
    "### Importing the required libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755ddad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')     #download if using this module for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24450d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16956d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#For Gensim\n",
    "import gensim\n",
    "import string\n",
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc8083",
   "metadata": {},
   "source": [
    "## Bag-of-words \n",
    "Bag-of-words is a simplistic method for identifying topics in a documents. It works on the assumption that the higher the frequency of the term, the higher it's importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4d1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War was a 2018 American superhero film based on the Marvel Comics superhero team the Avengers. It is the 19th film in the Marvel Cinematic Universe (MCU). The running time of the movie was 149 minutes and the box office collection was around 2 billion dollars. (Source: Wikipedia)\n"
     ]
    }
   ],
   "source": [
    "text1  =  \"Avengers: Infinity War was a 2018 American superhero film based on the Marvel Comics superhero team the Avengers. It is the 19th film in the Marvel Cinematic Universe (MCU). The running time of the movie was 149 minutes and the box office collection was around 2 billion dollars. (Source: Wikipedia)\"\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ea5559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a3f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avengers', ':', 'infinity', 'war', 'was', 'a', '2018', 'american', 'superhero', 'film', 'based', 'on', 'the', 'marvel', 'comics', 'superhero', 'team', 'the', 'avengers', '.', 'it', 'is', 'the', '19th', 'film', 'in', 'the', 'marvel', 'cinematic', 'universe', '(', 'mcu', ')', '.', 'the', 'running', 'time', 'of', 'the', 'movie', 'was', '149', 'minutes', 'and', 'the', 'box', 'office', 'collection', 'was', 'around', '2', 'billion', 'dollars', '.', '(', 'source', ':', 'wikipedia', ')']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text1)\n",
    "lowercase_tokens = [t.lower() for t in tokens]\n",
    "print(lowercase_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f3be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 7), ('was', 3), ('.', 3), ('avengers', 2), (':', 2), ('superhero', 2), ('film', 2), ('marvel', 2), ('(', 2), (')', 2)]\n"
     ]
    }
   ],
   "source": [
    "bagofwords_1 = Counter(lowercase_tokens)\n",
    "print(bagofwords_1.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429643f",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f8b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avengers', 'infinity', 'war', 'american', 'superhero', 'film', 'based', 'marvel', 'comics', 'superhero', 'team', 'avengers', 'film', 'marvel', 'cinematic', 'universe', 'mcu', 'running', 'time', 'movie', 'minutes', 'box', 'office', 'collection', 'around', 'billion', 'dollars', 'source', 'wikipedia']\n"
     ]
    }
   ],
   "source": [
    "alphabets = [t for t in lowercase_tokens if t.isalpha()]\n",
    "\n",
    "words = stopwords.words(\"english\")\n",
    "stopwords_revoked = [t for t in alphabets if t not in words]\n",
    "\n",
    "print(stopwords_revoked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274711b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('avenger', 2), ('superhero', 2), ('film', 2), ('marvel', 2), ('infinity', 1), ('war', 1)]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_tokens = [lemmatizer.lemmatize(t) for t in stopwords_revoked]\n",
    "\n",
    "bags_words = Counter(lem_tokens)\n",
    "print(bags_words.most_common(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d619a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
