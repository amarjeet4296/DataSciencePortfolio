{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85f8431",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0218ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1a291493e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6ea01",
   "metadata": {},
   "source": [
    "## Strings to Hashes\n",
    "\n",
    "You're aware that whenever you created a doc, the words of the doc are stored in the **vocab**.\n",
    "\n",
    "Think like that you have about 1000 text documents each having information about various clothing items of different brands. The chances are, the words \"shirt\" and \"pants\" are going to be very common. Each time the word \"shirt\" occurs, if spaCy were to store the exact string, you'll end up losing huge memory space.\n",
    "\n",
    "But this doesn't happen. why?\n",
    "\n",
    "spaCy **hashes** or converts each string to a unique ID that is stored in the $StringStore$.\n",
    "\n",
    "But, what is StringStore?\n",
    "\n",
    "It's a dictionary **mapping of hash value to string**, for example 10543432924755684266 -> box\n",
    "\n",
    "You can print the hash value if you know the string and vice-versa. This is contained in **nlp.vocab.strings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5b36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strings to hashes and back\n",
    "doc = nlp(\"I love travelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecadfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5902765392174988614\n"
     ]
    }
   ],
   "source": [
    "# Look up the hash for the word \"travelling\"\n",
    "\n",
    "word_hash = nlp.vocab.strings[\"travelling\"]\n",
    "print(word_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3a28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travelling\n"
     ]
    }
   ],
   "source": [
    "# Look up the word_hash to get the string\n",
    "word_string = nlp.vocab.strings[word_hash]\n",
    "print(word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bd534",
   "metadata": {},
   "source": [
    "Interestingly, a word will have the same hash value irrespective of which document it occurs in or which spacy model is being used.\n",
    "\n",
    "So your results are reproducible even if you run your code in some one else's machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f778817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Doc 1----------\n",
      "Raymond   5945540083247941101\n",
      "shirts   9181315343169869855\n",
      "are   5012629990875267006\n",
      "famous   17809293829314912000\n",
      "----------Doc 2-----------\n",
      "I   4690420944186131903\n",
      "washed   5520327350569975027\n",
      "my   227504873216781231\n",
      "shirts   9181315343169869855\n"
     ]
    }
   ],
   "source": [
    "# Create two different doc with a common word\n",
    "doc1 = nlp('Raymond shirts are famous')\n",
    "doc2 = nlp('I washed my shirts')\n",
    "\n",
    "print('--------Doc 1----------')\n",
    "for token in doc1:\n",
    "    hash_value = nlp.vocab.strings[token.text]\n",
    "    print(token.text, \" \", hash_value)\n",
    "\n",
    "print('----------Doc 2-----------')\n",
    "for token in doc2:\n",
    "    hash_value = nlp.vocab.strings[token.text]\n",
    "    print(token.text, \" \", hash_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b9f56",
   "metadata": {},
   "source": [
    "## Lexical attributes of spaCy\n",
    "\n",
    "Recall that we used **is_punct** and **is_space** attributes in Text Preprocessing. They are called as **'lexical attributes'**\n",
    "\n",
    "we will learn about a few more significant lexical attributes.\n",
    "\n",
    "The spaCy model provides many useful lexical attributes. These are the attributes of token object, that give you information on the type of token.\n",
    "\n",
    "e.g : **like_num** attributes of a token to check if it is a number. Let's print all the numbers in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5e3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens which are like numbers\n",
    "\n",
    "text = '2020 is far worse than 2009'\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1fc4e",
   "metadata": {},
   "source": [
    "Some real-life applications of these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b52a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Production = \"Production in chennai is 87%. In kolkata it as low as 43%. In Bangalore, production is as good as 98%. In jaipur, production is average around 78%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d641d12",
   "metadata": {},
   "source": [
    "List of various percentages in the text.\n",
    "\n",
    "We can convert the text into a **Doc** object of spaCy and check what tokens are numbers through like_num attribute. If it is a number, we can check if the next token is \"%\". we can access the index of next token through token. i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10c649a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "43\n",
      "98\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "#Finding the tokens which are number followed by %\n",
    "\n",
    "production_doc = nlp(Production)\n",
    "for token in production_doc:\n",
    "    if token.like_num:\n",
    "        index_of_next_token = token.i + 1\n",
    "        next_token = production_doc[index_of_next_token]\n",
    "        if next_token.text == \"%\":\n",
    "            print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7c118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
