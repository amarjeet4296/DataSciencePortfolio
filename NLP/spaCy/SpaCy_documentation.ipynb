{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50008b52",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "spaCy is an advanced modern library for Natural Language Processing. This document will guide to learn how to use spaCy for various tasks. It's open source and use for industrial grade.\n",
    "1. spaCy comes with pretrained NLP models that can perform most common NLP tasks, such as tokenization, parts of speech(POS)tagging, named entity recognition(ner), lemmatization, transforming word vectors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe2785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1147f",
   "metadata": {},
   "source": [
    "if you're dealing with a particular language, you can load the spacy model specific to the language using spacy.load() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f797d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1e84164ca30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load small english model: https://spacy.io/models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd86ed",
   "metadata": {},
   "source": [
    "This returns a Language object that comes ready with multiple built-in-capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503de2d9",
   "metadata": {},
   "source": [
    "### The Doc object\n",
    "Let us say you have your text data in a string. what can be done to understand the structure of the text? First, call the load nlp object on the text. It should return a processes Doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8804025b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parse text through the 'nlp' model\n",
    "my_text = ''' The economic situation of the country is on edge, as the stock market crashed causing loss of millions. Citizens who had their main investment in the share-market are facing a great loss. Many companies might lay off thousands of people to reducelabor cost'''\n",
    "my_doc = nlp(my_text)\n",
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00059c2",
   "metadata": {},
   "source": [
    "what exactly is a Doc object?\n",
    "\n",
    "It is a sequence of tokens that contains not just the original text but all the results produced by the spaCy model after processing the text. Useful information such as the lemma of the text, whether it is a stop word or not, named entities, the word vector of the text and so on are pre-computed and readily stored in the Doc object.\n",
    "The good thing is that you have complete control on what information needs to be pre-computed and customized. We will see all of that shortly.\n",
    "\n",
    "And, through the text gets split into tokens, no information of the original text is actually lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459b080",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9bcbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The\n",
      "economic\n",
      "situation\n",
      "of\n",
      "the\n",
      "country\n",
      "is\n",
      "on\n",
      "edge\n",
      ",\n",
      "as\n",
      "the\n",
      "stock\n",
      "market\n",
      "crashed\n",
      "causing\n",
      "loss\n",
      "of\n",
      "millions\n",
      ".\n",
      "Citizens\n",
      "who\n",
      "had\n",
      "their\n",
      "main\n",
      "investment\n",
      "in\n",
      "the\n",
      "share\n",
      "-\n",
      "market\n",
      "are\n",
      "facing\n",
      "a\n",
      "great\n",
      "loss\n",
      ".\n",
      "Many\n",
      "companies\n",
      "might\n",
      "lay\n",
      "off\n",
      "thousands\n",
      "of\n",
      "people\n",
      "to\n",
      "reducelabor\n",
      "cost\n"
     ]
    }
   ],
   "source": [
    "# Printing the tokens of a doc\n",
    "for token in my_doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bec6b",
   "metadata": {},
   "source": [
    "##### What is Tokenization\n",
    "\n",
    "Tokenization is the process of converting a text into smaller sub-text, based on certain predefined rules. For example, sentences are tokenized to words(and punctuation optionally). And paragraphs into sentences, depending on the context.\n",
    "\n",
    "This is typically the first step for NLP tasks like text classification, sentiment analysis, etc. Each token in spacy has different attributes that tell us a great deal of information.\n",
    "\n",
    "The above tokens contains punctuation and common words like \"a\", \"the\", \"was\" etc. These do not add any value to the meaning of your text. They are called stop words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb650e",
   "metadata": {},
   "source": [
    "### Text-Preprocessing with spaCy\n",
    "\n",
    "There is 'noise' in the tokens. you have punctuation like commas, brackets, full stop and some extra while spaces too. The process of removing noise from the noise from the doc is called $Text-Cleaning$ or $Preprocessing$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37ff44",
   "metadata": {},
   "source": [
    "#### What is the need for text preprocessing?\n",
    "\n",
    "The outcome of the NLP task you perform be it classification, finding sentiments, topic modelling etc the quality of the output depends heavily on the quality of the input text used\n",
    "\n",
    "Stop words and punctuation usually (not always) don't add value to the meaning of the text and can potentially impact the outcome. To avoid this, its might make sense to remove them and clean the text of unwanted characters can reduce the size of the corpus.\n",
    "\n",
    "##### How to identify and remove the stopwords and punctuation?\n",
    "\n",
    "The tokens in spaCy have attributes which will help you identify if it is a stop word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34b9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- False --- False\n",
      "The -- True --- False\n",
      "economic -- False --- False\n",
      "situation -- False --- False\n",
      "of -- True --- False\n",
      "the -- True --- False\n",
      "country -- False --- False\n",
      "is -- True --- False\n",
      "on -- True --- False\n",
      "edge -- False --- False\n",
      ", -- False --- True\n",
      "as -- True --- False\n",
      "the -- True --- False\n",
      "stock -- False --- False\n",
      "market -- False --- False\n",
      "crashed -- False --- False\n",
      "causing -- False --- False\n",
      "loss -- False --- False\n",
      "of -- True --- False\n",
      "millions -- False --- False\n",
      ". -- False --- True\n",
      "Citizens -- False --- False\n",
      "who -- True --- False\n",
      "had -- True --- False\n",
      "their -- True --- False\n",
      "main -- False --- False\n",
      "investment -- False --- False\n",
      "in -- True --- False\n",
      "the -- True --- False\n",
      "share -- False --- False\n",
      "- -- False --- True\n",
      "market -- False --- False\n",
      "are -- True --- False\n",
      "facing -- False --- False\n",
      "a -- True --- False\n",
      "great -- False --- False\n",
      "loss -- False --- False\n",
      ". -- False --- True\n",
      "Many -- True --- False\n",
      "companies -- False --- False\n",
      "might -- True --- False\n",
      "lay -- False --- False\n",
      "off -- True --- False\n",
      "thousands -- False --- False\n",
      "of -- True --- False\n",
      "people -- False --- False\n",
      "to -- True --- False\n",
      "reducelabor -- False --- False\n",
      "cost -- False --- False\n"
     ]
    }
   ],
   "source": [
    "#Printing tokens and boolean values stored in different attributes.\n",
    "for token in my_doc:\n",
    "    print(token.text, \"--\", token.is_stop, \"---\", token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49665c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "economic\n",
      "situation\n",
      "country\n",
      "edge\n",
      "stock\n",
      "market\n",
      "crashed\n",
      "causing\n",
      "loss\n",
      "millions\n",
      "Citizens\n",
      "main\n",
      "investment\n",
      "share\n",
      "market\n",
      "facing\n",
      "great\n",
      "loss\n",
      "companies\n",
      "lay\n",
      "thousands\n",
      "people\n",
      "reducelabor\n",
      "cost\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopwords and punctuations\n",
    "\n",
    "my_doc_cleaned = [token for token in my_doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "for token in my_doc_cleaned:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899374b",
   "metadata": {},
   "source": [
    "The computational costs decreased costs decreased by a great amount due to reduce in the number of tokens. In order to grasp the effect the effect of preprocessing on large text data, you can exclude the below code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b174c",
   "metadata": {},
   "source": [
    "## spaCy pipelines\n",
    "\n",
    "you have used tokens and docs in many ways till now. In this section, let's dive deeper understand the basic pipeline behind this.\n",
    "\n",
    "When you call the nlp object on spaCy, the text is segmented into tokensto create a Doc object. Following this, various process are carried out on the Doc to add the attributes like POS tags, Lemma tags, dependency tags etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af9018",
   "metadata": {},
   "source": [
    "### What are pipeline components?\n",
    "\n",
    "The processing pipeline consists of components. where each component perform it's task and passes the Processed Doc to the next component. These are called as Pipeline Components\n",
    "\n",
    "spaCy provides certain in-built pipeline components. Let's look at them\n",
    "\n",
    "The built-in pipeline components of spacy are:\n",
    "\n",
    "$Tokenizer$ : It is responsible for segmenting the text into tokens are turning a Doc object. This the first and compulsory step in a pipeline.\n",
    "\n",
    "$Tagger$ : It is responsible for assigning Part-of-speech tags. It takes a Doc as input and creates $Doc[i].tag$\n",
    "\n",
    "$Dependency$ : It is known as parser. It is responsible for assigning the dependency tags to each token. It takes a Doc as input and returns  the processed Doc\n",
    "\n",
    "$Entity Recognizer$ : This component is reffered as $ner$. It is responsible for identifying named entities and assigning labels to them.\n",
    "\n",
    "$Text Categorizer$ : This component is called $textcat$. It will assign categories to Docs.\n",
    "\n",
    "$Entity Ruler$ : This component is called *entity_ruler*.It is responsible for assigningnamed entitle based on pattern rules. Revisit Rule Based Matching to know more.\n",
    "\n",
    "$Sentencizer$ : This component is called **sentencizer** and can perform rule based sentence segmentation.\n",
    "\n",
    "**merge_noun_chunks** : It is called **mergenounchunk**. This component is responsible for merging all noun chunks into a single token. It has to be add in the pipeline after **tagger** and **parser**\n",
    "\n",
    "**merge_entities** :  It is called **merge_entities**. This component can merge all entities into a single token. It has to added after the **ner**\n",
    "\n",
    "**merge_subtokens** : It is called **merge_subtokens**. This components can merge the subtokens into a single token. \n",
    "\n",
    "These are the various in-built pipeline components. It is not necessary for every spaCy model to have each of the above components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7b0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
